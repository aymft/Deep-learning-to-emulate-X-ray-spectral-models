{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation des modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.ticker as ticker\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Matplotlib settings\n",
    "\n",
    "from matplotlib import rcParams\n",
    "\n",
    "# Scikit-learn modules\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow & Keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import Huber\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Keras Tuner\n",
    "import keras_tuner as kt\n",
    "\n",
    "# Custom PCA module\n",
    "import pcax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement des donnÃ©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape: (100000, 2)\n",
      "Spectrum shape: (100000, 10000)\n",
      "Variance explained by first 15 components: 100.0000%\n"
     ]
    }
   ],
   "source": [
    "def load_data(file_path):\n",
    "    data = np.load(file_path)\n",
    "    spectres = np.log1p(data[\"simulations\"])  # Log transform for stability\n",
    "    theta = data[\"theta\"]\n",
    "    return spectres, theta\n",
    "\n",
    "# Update with your own path\n",
    "data_path = Path(\"/Users/xifumacbook/Documents/Codes/Spectres/diskbb/diskbb06-001-50.npz\")\n",
    "spectres, theta = load_data(data_path)\n",
    "\n",
    "print(f\"Theta shape: {theta.shape}\")\n",
    "print(f\"Spectrum shape: {spectres.shape}\")\n",
    "\n",
    "# Extract temperature parameter and reshape\n",
    "temperatures = theta[:, 0].reshape(-1, 1)\n",
    "\n",
    "# =======================\n",
    "# Apply Normalization using Spline\n",
    "# =======================\n",
    "\n",
    "# Load spline for normalization\n",
    "spline_filename = data_path.with_suffix(\".pkl\")\n",
    "with open(spline_filename, \"rb\") as f:\n",
    "    spline_N = pickle.load(f)\n",
    "\n",
    "# Function to get adjusted normalization\n",
    "def get_norm_spline(T):\n",
    "    return 10**spline_N(np.log10(T))\n",
    "\n",
    "# =======================\n",
    "# Split Data\n",
    "# =======================\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    temperatures, spectres, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "spectre_scaler = StandardScaler()\n",
    "\n",
    "y_train_scaled = spectre_scaler.fit_transform(y_train)\n",
    "y_test_scaled = spectre_scaler.transform(y_test)\n",
    "\n",
    "# =======================\n",
    "# PCA Reduction\n",
    "# =======================\n",
    "\n",
    "n_PCA = 15\n",
    "pca_state = pcax.fit(y_train_scaled, n_components=n_PCA)\n",
    "y_train_pca = pcax.transform(pca_state, y_train_scaled)\n",
    "y_test_pca = pcax.transform(pca_state, y_test_scaled)\n",
    "\n",
    "explained_variance = np.sum(y_train_pca.var(axis=0)) / np.sum(y_train_scaled.var(axis=0))\n",
    "print(f\"Variance explained by first {n_PCA} components: {explained_variance:.4%}\")\n",
    "\n",
    "# # Save PCA state\n",
    "# with open(\"pca_state.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pca_state, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 63 Complete [00h 06m 38s]\n",
      "val_loss: 0.001094851759262383\n",
      "\n",
      "Best val_loss So Far: 0.0003629798593465239\n",
      "Total elapsed time: 05h 21m 50s\n",
      "\n",
      "Search: Running Trial #64\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "24                |112               |units_1\n",
      "relu              |swish             |activation_1\n",
      "False             |True              |use_second_layer\n",
      "False             |True              |use_third_layer\n",
      "0.0035372         |0.0064859         |learning_rate\n",
      "104               |40                |units_3\n",
      "tanh              |swish             |activation_3\n",
      "40                |16                |units_2\n",
      "relu              |swish             |activation_2\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - loss: 0.0120 - val_loss: 0.0098 - learning_rate: 0.0035\n",
      "Epoch 2/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0093 - val_loss: 0.0088 - learning_rate: 0.0035\n",
      "Epoch 3/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0085 - val_loss: 0.0081 - learning_rate: 0.0035\n",
      "Epoch 4/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0078 - val_loss: 0.0074 - learning_rate: 0.0035\n",
      "Epoch 5/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0072 - val_loss: 0.0070 - learning_rate: 0.0035\n",
      "Epoch 6/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0068 - val_loss: 0.0067 - learning_rate: 0.0035\n",
      "Epoch 7/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0065 - val_loss: 0.0064 - learning_rate: 0.0035\n",
      "Epoch 8/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0062 - val_loss: 0.0062 - learning_rate: 0.0035\n",
      "Epoch 9/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0061 - val_loss: 0.0060 - learning_rate: 0.0035\n",
      "Epoch 10/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0059 - val_loss: 0.0058 - learning_rate: 0.0035\n",
      "Epoch 11/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0057 - val_loss: 0.0057 - learning_rate: 0.0035\n",
      "Epoch 12/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0056 - val_loss: 0.0056 - learning_rate: 0.0035\n",
      "Epoch 13/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0054 - val_loss: 0.0054 - learning_rate: 0.0035\n",
      "Epoch 14/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0053 - val_loss: 0.0053 - learning_rate: 0.0035\n",
      "Epoch 15/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0052 - val_loss: 0.0052 - learning_rate: 0.0035\n",
      "Epoch 16/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0050 - val_loss: 0.0051 - learning_rate: 0.0035\n",
      "Epoch 17/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0050 - val_loss: 0.0050 - learning_rate: 0.0035\n",
      "Epoch 18/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0049 - val_loss: 0.0049 - learning_rate: 0.0035\n",
      "Epoch 19/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0048 - val_loss: 0.0048 - learning_rate: 0.0035\n",
      "Epoch 20/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0048 - val_loss: 0.0048 - learning_rate: 0.0035\n",
      "Epoch 21/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0047 - val_loss: 0.0047 - learning_rate: 0.0035\n",
      "Epoch 22/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0046 - val_loss: 0.0046 - learning_rate: 0.0035\n",
      "Epoch 23/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0045 - val_loss: 0.0046 - learning_rate: 0.0035\n",
      "Epoch 24/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0045 - learning_rate: 0.0035\n",
      "Epoch 25/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0045 - learning_rate: 0.0035\n",
      "Epoch 26/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0044 - val_loss: 0.0044 - learning_rate: 0.0035\n",
      "Epoch 27/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0035\n",
      "Epoch 28/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0043 - val_loss: 0.0044 - learning_rate: 0.0035\n",
      "Epoch 29/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0043 - learning_rate: 0.0035\n",
      "Epoch 30/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0043 - learning_rate: 0.0035\n",
      "Epoch 31/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0042 - val_loss: 0.0042 - learning_rate: 0.0035\n",
      "Epoch 32/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.0042 - learning_rate: 0.0035\n",
      "Epoch 33/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0042 - learning_rate: 0.0035\n",
      "Epoch 34/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0041 - val_loss: 0.0041 - learning_rate: 0.0035\n",
      "Epoch 35/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0041 - learning_rate: 0.0035\n",
      "Epoch 36/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - loss: 0.0040 - val_loss: 0.0041 - learning_rate: 0.0035\n",
      "Epoch 37/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0035\n",
      "Epoch 38/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0040 - val_loss: 0.0040 - learning_rate: 0.0035\n",
      "Epoch 39/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 0.0035\n",
      "Epoch 40/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0040 - learning_rate: 0.0035\n",
      "Epoch 41/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 0.0035\n",
      "Epoch 42/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0039 - val_loss: 0.0039 - learning_rate: 0.0035\n",
      "Epoch 43/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0039 - learning_rate: 0.0035\n",
      "Epoch 44/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0035\n",
      "Epoch 45/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0035\n",
      "Epoch 46/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0038 - learning_rate: 0.0035\n",
      "Epoch 47/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0038 - learning_rate: 0.0035\n",
      "Epoch 48/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 0.0035\n",
      "Epoch 49/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0037 - learning_rate: 0.0035\n",
      "Epoch 50/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 0.0035\n",
      "Epoch 51/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0037 - learning_rate: 0.0035\n",
      "Epoch 52/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - loss: 0.0036 - val_loss: 0.0036 - learning_rate: 0.0035\n",
      "Epoch 53/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0036 - val_loss: 0.0036 - learning_rate: 0.0035\n",
      "Epoch 54/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - loss: 0.0035 - val_loss: 0.0036 - learning_rate: 0.0035\n",
      "Epoch 55/100\n",
      "\u001b[1m317/317\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - loss: 0.0035 - val_loss: 0.0036 - learning_rate: 0.0035\n",
      "Epoch 56/100\n",
      "\u001b[1m 82/317\u001b[0m \u001b[32mâ”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0035"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 47\u001b[0m\n\u001b[1;32m     37\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mRandomSearch(\n\u001b[1;32m     38\u001b[0m     build_model,\n\u001b[1;32m     39\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspectre_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# ðŸ“Œ Recherche des meilleurs hyperparamÃ¨tres\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_pca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrestore_best_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mReduceLROnPlateau\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfactor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_lr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-6\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     57\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# ðŸ“Œ RÃ©cupÃ©ration du meilleur modÃ¨le\u001b[39;00m\n\u001b[1;32m     60\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py:372\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    370\u001b[0m callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m    371\u001b[0m logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[0;32m--> 372\u001b[0m \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_training:\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/callbacks/callback_list.py:172\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_async_dispatch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_train_batch_end, batch, logs)\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/callbacks/callback_list.py:194\u001b[0m, in \u001b[0;36mCallbackList._on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    192\u001b[0m logs \u001b[38;5;241m=\u001b[39m python_utils\u001b[38;5;241m.\u001b[39mpythonify_logs(logs)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[0;32m--> 194\u001b[0m     \u001b[43mcallback\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_end\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/callbacks/progbar_logger.py:58\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mon_train_batch_end\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 58\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_progbar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/callbacks/progbar_logger.py:95\u001b[0m, in \u001b[0;36mProgbarLogger._update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseen \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# One-indexed.\u001b[39;00m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 95\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprogbar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlogs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/utils/progbar.py:182\u001b[0m, in \u001b[0;36mProgbar.update\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m finalize:\n\u001b[1;32m    180\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 182\u001b[0m \u001b[43mio_utils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprint_msg\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline_break\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prev_total_width \u001b[38;5;241m=\u001b[39m total_width\n\u001b[1;32m    184\u001b[0m message \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/utils/io_utils.py:98\u001b[0m, in \u001b[0;36mprint_msg\u001b[0;34m(message, line_break)\u001b[0m\n\u001b[1;32m     96\u001b[0m message \u001b[38;5;241m=\u001b[39m message \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m line_break \u001b[38;5;28;01melse\u001b[39;00m message\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 98\u001b[0m     \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeEncodeError\u001b[39;00m:\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m# If the encoding differs from UTF-8, `sys.stdout.write` may fail.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m# To address this, replace special unicode characters in the\u001b[39;00m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# message, and then encode and decode using the target encoding.\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     message \u001b[38;5;241m=\u001b[39m _replace_special_unicode_character(message)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/ipykernel/iostream.py:694\u001b[0m, in \u001b[0;36mOutStream.write\u001b[0;34m(self, string)\u001b[0m\n\u001b[1;32m    692\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpub_thread\u001b[38;5;241m.\u001b[39mschedule(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_schedule_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(string)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/ipykernel/iostream.py:590\u001b[0m, in \u001b[0;36mOutStream._schedule_flush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_schedule_in_thread\u001b[39m():\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_io_loop\u001b[38;5;241m.\u001b[39mcall_later(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflush_interval, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush)\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpub_thread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mschedule\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_schedule_in_thread\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/ipykernel/iostream.py:267\u001b[0m, in \u001b[0;36mIOPubThread.schedule\u001b[0;34m(self, f)\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_events\u001b[38;5;241m.\u001b[39mappend(f)\n\u001b[1;32m    266\u001b[0m     \u001b[38;5;66;03m# wake event thread (message content is ignored)\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_event_pipe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     f()\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/zmq/sugar/socket.py:701\u001b[0m, in \u001b[0;36mSocket.send\u001b[0;34m(self, data, flags, copy, track, routing_id, group)\u001b[0m\n\u001b[1;32m    694\u001b[0m         data \u001b[38;5;241m=\u001b[39m zmq\u001b[38;5;241m.\u001b[39mFrame(\n\u001b[1;32m    695\u001b[0m             data,\n\u001b[1;32m    696\u001b[0m             track\u001b[38;5;241m=\u001b[39mtrack,\n\u001b[1;32m    697\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    698\u001b[0m             copy_threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy_threshold,\n\u001b[1;32m    699\u001b[0m         )\n\u001b[1;32m    700\u001b[0m     data\u001b[38;5;241m.\u001b[39mgroup \u001b[38;5;241m=\u001b[39m group\n\u001b[0;32m--> 701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrack\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m_zmq.py:1092\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1140\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq.Socket.send\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:1339\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._send_copy\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_zmq.py:160\u001b[0m, in \u001b[0;36mzmq.backend.cython._zmq._check_rc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# def build_model(hp):\n",
    "#     model = Sequential()\n",
    "\n",
    "#     # ðŸ“Œ PremiÃ¨re couche cachÃ©e (nombre de neurones variable)\n",
    "#     model.add(Dense(\n",
    "#         units=hp.Int(\"units_1\", min_value=8, max_value=256, step=8),\n",
    "#         activation=hp.Choice(\"activation_1\", [\"relu\", \"gelu\", \"tanh\", \"swish\"]),\n",
    "#         input_dim=X_train.shape[1]\n",
    "#     ))\n",
    "\n",
    "#     # ðŸ“Œ Ajout possible d'une deuxiÃ¨me couche cachÃ©e\n",
    "#     if hp.Boolean(\"use_second_layer\"):\n",
    "#         model.add(Dense(\n",
    "#             units=hp.Int(\"units_2\", min_value=8, max_value=256, step=8),\n",
    "#             activation=hp.Choice(\"activation_2\", [\"relu\", \"gelu\", \"tanh\", \"swish\"])\n",
    "#         ))\n",
    "\n",
    "#     # ðŸ“Œ Ajout possible d'une troisiÃ¨me couche cachÃ©e\n",
    "#     if hp.Boolean(\"use_third_layer\"):\n",
    "#         model.add(Dense(\n",
    "#             units=hp.Int(\"units_3\", min_value=8, max_value=256, step=8),\n",
    "#             activation=hp.Choice(\"activation_3\", [\"relu\", \"gelu\", \"tanh\", \"swish\"])\n",
    "#         ))\n",
    "\n",
    "#     # ðŸ“Œ Couche de sortie\n",
    "#     model.add(Dense(n_PCA, activation=\"linear\"))\n",
    "\n",
    "#     # ðŸ“Œ Compilation du modÃ¨le\n",
    "#     model.compile(\n",
    "#         optimizer=Adam(learning_rate=hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-2, sampling=\"log\")),\n",
    "#         loss=Huber(delta=0.001)\n",
    "#     )\n",
    "\n",
    "#     return model\n",
    "\n",
    "# # ðŸ“Œ DÃ©finition du tuner (Random Search)\n",
    "# tuner = kt.RandomSearch(\n",
    "#     build_model,\n",
    "#     objective=\"val_loss\",\n",
    "#     max_trials=100,  # Nombre dâ€™architectures testÃ©es\n",
    "#     executions_per_trial=2,  # Moyenne sur plusieurs exÃ©cutions pour Ã©viter la variance\n",
    "#     directory=\"tuner_results\",\n",
    "#     project_name=\"spectre_tuning\"\n",
    "# )\n",
    "\n",
    "# # ðŸ“Œ Recherche des meilleurs hyperparamÃ¨tres\n",
    "# tuner.search(\n",
    "#     X_train, y_train_pca,\n",
    "#     validation_split=0.1,\n",
    "#     epochs=100,\n",
    "#     batch_size=128,\n",
    "#     callbacks=[\n",
    "#         EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=True, verbose=1),\n",
    "#         ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "#     ],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# # ðŸ“Œ RÃ©cupÃ©ration du meilleur modÃ¨le\n",
    "# best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "# print(f\"âœ… Meilleurs hyperparamÃ¨tres trouvÃ©s :\\n{best_hps.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 90 Complete [00h 13m 57s]\n",
      "val_loss: 3.9085490703582764\n",
      "\n",
      "Best val_loss So Far: 0.0002678835589904338\n",
      "Total elapsed time: 01h 19m 51s\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# Hyperparameter Tuning with Keras Tuner\n",
    "# =======================\n",
    "\n",
    "def build_tuned_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int(\"units_1\", min_value=8, max_value=256, step=8),\n",
    "        activation=hp.Choice(\"activation_1\", [\"relu\", \"gelu\", \"tanh\", \"swish\"]),\n",
    "        input_dim=X_train.shape[1]\n",
    "    ))\n",
    "\n",
    "    # ðŸ“Œ Optional second hidden layer\n",
    "    if hp.Boolean(\"use_second_layer\"):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(\"units_2\", min_value=8, max_value=256, step=8),\n",
    "            activation=hp.Choice(\"activation_2\", [\"relu\", \"gelu\", \"tanh\", \"swish\"])\n",
    "        ))\n",
    "\n",
    "    # # ðŸ“Œ Optional third hidden layer\n",
    "    # if hp.Boolean(\"use_third_layer\"):\n",
    "    #     model.add(Dense(\n",
    "    #         units=hp.Int(\"units_3\", min_value=8, max_value=256, step=8),\n",
    "    #         activation=hp.Choice(\"activation_3\", [\"relu\", \"gelu\", \"tanh\", \"swish\"])\n",
    "    #     ))\n",
    "\n",
    "    # ðŸ“Œ Output layer\n",
    "    model.add(Dense(n_PCA, activation=\"linear\"))\n",
    "\n",
    "    # ðŸ“Œ Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-2, sampling=\"log\")),\n",
    "        loss=Huber(delta=1.35)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_tuned_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir2',\n",
    "    project_name='spectra_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, y_train_pca,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the optimal hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Meilleurs hyperparamÃ¨tres trouvÃ©s :\n",
      "{'units_1': 16, 'activation_1': 'gelu', 'use_second_layer': True, 'learning_rate': 0.005058857837580419, 'units_2': 160, 'activation_2': 'gelu', 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0046'}\n"
     ]
    }
   ],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"âœ… Meilleurs hyperparamÃ¨tres trouvÃ©s :\\n{best_hps.values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta shape: (100000, 2)\n",
      "Spectrum shape: (100000, 10000)\n",
      "Variance explained by first 15 components: 99.9999%\n",
      "Reloading Tuner from keras_tuner_dir2/spectra_tuning/tuner0.json\n",
      "âœ… Meilleurs hyperparamÃ¨tres : {'units_1': 16, 'activation_1': 'gelu', 'use_second_layer': True, 'learning_rate': 0.005058857837580419, 'units_2': 160, 'activation_2': 'gelu', 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 3, 'tuner/round': 3, 'tuner/trial_id': '0046'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/xifumacbook/miniforge3/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m313/313\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10000,15) (10000,) (10000,15) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 211\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Build and evaluate the best model\u001b[39;00m\n\u001b[1;32m    210\u001b[0m best_model \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mbuild(best_hps)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mbest_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_pca\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_on_global_error\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m    217\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# Plot the global errors\u001b[39;00m\n\u001b[1;32m    220\u001b[0m plot_global_errors(best_model, X_test, y_test_pca, pca, spectre_scaler)\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[14], line 95\u001b[0m, in \u001b[0;36mStopOnGlobalError.on_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m     93\u001b[0m y_pred_scaled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpca\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_pca)\n\u001b[1;32m     94\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectre_scaler\u001b[38;5;241m.\u001b[39minverse_transform(y_pred_scaled)\n\u001b[0;32m---> 95\u001b[0m y_test_original \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspectre_scaler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43my_test_scaled\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m global_errors \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m true_spectrum, pred_spectrum \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(np\u001b[38;5;241m.\u001b[39mexpm1(y_test_original), np\u001b[38;5;241m.\u001b[39mexpm1(y_pred)):\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.12/site-packages/sklearn/preprocessing/_data.py:1125\u001b[0m, in \u001b[0;36mStandardScaler.inverse_transform\u001b[0;34m(self, X, copy)\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_std:\n\u001b[0;32m-> 1125\u001b[0m         \u001b[43mX\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscale_\u001b[49m\n\u001b[1;32m   1126\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_mean:\n\u001b[1;32m   1127\u001b[0m         X \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmean_\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10000,15) (10000,) (10000,15) "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras_tuner as kt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# =======================\n",
    "# Data Loading\n",
    "# =======================\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = np.load(file_path)\n",
    "    spectres = np.log1p(data[\"simulations\"])  # Log transform for stability\n",
    "    theta = data[\"theta\"]\n",
    "    return spectres, theta\n",
    "\n",
    "# Update with your own path\n",
    "data_path = Path(\"/Users/xifumacbook/Documents/Codes/Spectres/diskbb/diskbb06-001-50.npz\")\n",
    "spectres, theta = load_data(data_path)\n",
    "\n",
    "print(f\"Theta shape: {theta.shape}\")\n",
    "print(f\"Spectrum shape: {spectres.shape}\")\n",
    "\n",
    "# Extract temperature parameter and reshape\n",
    "temperatures = theta[:, 0].reshape(-1, 1)\n",
    "\n",
    "# =======================\n",
    "# Apply Normalization using Spline\n",
    "# =======================\n",
    "\n",
    "# Load spline for normalization\n",
    "spline_filename = data_path.with_suffix(\".pkl\")\n",
    "with open(spline_filename, \"rb\") as f:\n",
    "    spline_N = pickle.load(f)\n",
    "\n",
    "# Function to get adjusted normalization\n",
    "def get_norm_spline(T):\n",
    "    return 10**spline_N(np.log10(T))\n",
    "\n",
    "# =======================\n",
    "# Split Data\n",
    "# =======================\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    temperatures, spectres, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Scaling\n",
    "spectre_scaler = StandardScaler()\n",
    "y_train_scaled = spectre_scaler.fit_transform(y_train)\n",
    "y_test_scaled = spectre_scaler.transform(y_test)\n",
    "\n",
    "# =======================\n",
    "# PCA Reduction\n",
    "# =======================\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_PCA = 15\n",
    "pca = PCA(n_components=n_PCA)\n",
    "y_train_pca = pca.fit_transform(y_train_scaled)\n",
    "y_test_pca = pca.transform(y_test_scaled)\n",
    "\n",
    "explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "print(f\"Variance explained by first {n_PCA} components: {explained_variance:.4%}\")\n",
    "\n",
    "# =======================\n",
    "# Custom Callback to Stop on Global Error Threshold\n",
    "# =======================\n",
    "\n",
    "class StopOnGlobalError(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, X_test, y_test_scaled, pca, spectre_scaler, threshold=1.5):\n",
    "        super().__init__()\n",
    "        self.X_test = X_test\n",
    "        self.y_test_scaled = y_test_scaled\n",
    "        self.pca = pca\n",
    "        self.spectre_scaler = spectre_scaler\n",
    "        self.threshold = threshold\n",
    "        self.eps = 1e-8\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y_pred_pca = self.model.predict(self.X_test)\n",
    "        y_pred_scaled = self.pca.inverse_transform(y_pred_pca)\n",
    "        y_pred = self.spectre_scaler.inverse_transform(y_pred_scaled)\n",
    "        y_test_original = self.spectre_scaler.inverse_transform(self.y_test_scaled)\n",
    "\n",
    "        global_errors = []\n",
    "        for true_spectrum, pred_spectrum in zip(np.expm1(y_test_original), np.expm1(y_pred)):\n",
    "            global_error = 100 * np.linalg.norm(true_spectrum - pred_spectrum) / (np.linalg.norm(true_spectrum) + self.eps)\n",
    "            global_errors.append(global_error)\n",
    "\n",
    "        mean_global_error = np.mean(global_errors)\n",
    "        print(f\"\\nMean Global Error at epoch {epoch + 1}: {mean_global_error:.3f}%\")\n",
    "\n",
    "        if mean_global_error < self.threshold:\n",
    "            print(f\"\\nâœ… Stopping training as global error fell below {self.threshold}%\")\n",
    "            self.model.stop_training = True\n",
    "\n",
    "# =======================\n",
    "# Keras Tuner Integration\n",
    "# =======================\n",
    "\n",
    "def build_tuned_model(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(Dense(\n",
    "        units=hp.Int(\"units_1\", min_value=8, max_value=256, step=8),\n",
    "        activation=hp.Choice(\"activation_1\", [\"relu\", \"gelu\", \"tanh\", \"swish\"]),\n",
    "        input_dim=X_train.shape[1]\n",
    "    ))\n",
    "\n",
    "    # Optional hidden layer\n",
    "    if hp.Boolean(\"use_second_layer\"):\n",
    "        model.add(Dense(\n",
    "            units=hp.Int(\"units_2\", min_value=8, max_value=256, step=8),\n",
    "            activation=hp.Choice(\"activation_2\", [\"relu\", \"gelu\", \"tanh\", \"swish\"])\n",
    "        ))\n",
    "\n",
    "    # Output layer\n",
    "    model.add(Dense(n_PCA, activation=\"linear\"))\n",
    "\n",
    "    # Compile model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=hp.Float(\"learning_rate\", min_value=1e-5, max_value=1e-2, sampling=\"log\")),\n",
    "        loss=Huber(delta=1.35)\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "# Initialize the tuner\n",
    "tuner = kt.Hyperband(\n",
    "    build_tuned_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    factor=3,\n",
    "    directory='keras_tuner_dir2',\n",
    "    project_name='spectra_tuning'\n",
    ")\n",
    "\n",
    "# Early stopping on validation loss\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Custom callback to stop on global error < 1.5%\n",
    "stop_on_global_error = StopOnGlobalError(\n",
    "    X_test=X_test,\n",
    "    y_test_scaled=y_test_pca,\n",
    "    pca=pca,\n",
    "    spectre_scaler=spectre_scaler,\n",
    "    threshold=1.5\n",
    ")\n",
    "\n",
    "# Perform hyperparameter search\n",
    "tuner.search(\n",
    "    X_train, y_train_pca,\n",
    "    epochs=50,\n",
    "    validation_split=0.1,\n",
    "    callbacks=[early_stopping, stop_on_global_error],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "print(f\"âœ… Meilleurs hyperparamÃ¨tres : {best_hps.values}\")\n",
    "\n",
    "# =======================\n",
    "# Plot Global Errors Function\n",
    "# =======================\n",
    "\n",
    "def plot_global_errors(model, X_test, y_test_scaled, pca, spectre_scaler):\n",
    "    eps = 1e-8\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred_pca = model.predict(X_test)\n",
    "    y_pred_scaled = pca.inverse_transform(y_pred_pca)\n",
    "    y_pred = spectre_scaler.inverse_transform(y_pred_scaled)\n",
    "\n",
    "    # De-normalize test data\n",
    "    y_test_original = spectre_scaler.inverse_transform(y_test_scaled)\n",
    "\n",
    "    # Initialize list for global errors\n",
    "    global_errors = []\n",
    "    temperatures_test = X_test.flatten()\n",
    "\n",
    "    # Calculate global error for each spectrum\n",
    "    for true_spectrum, pred_spectrum in zip(np.expm1(y_test_original), np.expm1(y_pred)):\n",
    "        global_error = 100 * np.linalg.norm(true_spectrum - pred_spectrum) / (np.linalg.norm(true_spectrum) + eps)\n",
    "        global_errors.append(global_error)\n",
    "\n",
    "    # Plot global errors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(temperatures_test, global_errors, color='dodgerblue', alpha=0.8, edgecolor='black', s=50)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Temperature (keV)')\n",
    "    plt.ylabel('Global Error (%)')\n",
    "    plt.title('Global Errors Across Test Samples')\n",
    "    plt.show()\n",
    "\n",
    "# Build and evaluate the best model\n",
    "best_model = tuner.hypermodel.build(best_hps)\n",
    "best_model.fit(\n",
    "    X_train, y_train_pca,\n",
    "    validation_split=0.1,\n",
    "    epochs=100,\n",
    "    callbacks=[early_stopping, stop_on_global_error],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Plot the global errors\n",
    "plot_global_errors(best_model, X_test, y_test_pca, pca, spectre_scaler)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
